# -*- coding: utf-8 -*-
"""Comparative-Sentiment-Analysis-Using-VADER-and-BERT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wEOwpnl2btwXhZH_1vuHvBNZNyH_eZHn
"""

import pandas as pd
import numpy as np
import re
import string
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

from google.colab import files
uploaded  = files.upload()

df = pd.read_csv('amazon_reviews_sample.csv')
df.head()

df.info()

df["score"].value_counts(normalize = True)

plt.figure(figsize=(8,5))

sns.countplot(data=df,
              x='score',
              palette='viridis')

plt.title("DISTRIBUTION OF REVIEW SCORES", fontsize=12, weight = "bold")
plt.xlabel("SCORE", weight = "bold")
plt.ylabel("COUNT OF REVIEWS", weight = "bold")
plt.grid(True)
plt.show()

#Checking for inconcisistencies
def errors(data):
  errors = ["\[.*?\]","https?:\S+|www\.\S+","<.*?>+","\\n","\\w*\\d\\w*"]
  for i in errors:
      if df[data].str.contains(i, regex = True).any():
          print(f"Exists: {i}")
      else:
          print(f"Doesn't exist: {i}")

errors("review")

#Cleaning Text
import nltk
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('brown')
nltk.download('punkt_tab')
nltk.download('vader_lexicon')


lemmatizer = WordNetLemmatizer()
stopword = set(stopwords.words('english'))

def data_cleaner(text):
    text = str(text).lower()
    text = re.sub('\\[.*?\\]', '', text)                          #Square brackets
    text = re.sub('https?:\\S+|www\\.\\S+', '', text)             #Links
    text = re.sub('<.*?>+', '', text)                             #HTML TAGS
    text = re.sub(f"[{re.escape(string.punctuation)}]", '', text) #Punctuation
    text = re.sub('\\n', '', text)                                #Newline Chars
    text = re.sub('\\w*\\d\\w*', '', text)                        #Removes digits
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stopword]
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    clean_text = " ".join(tokens)
    return clean_text

df["Cleaned_Review"] = df["review"].astype(str).apply(data_cleaner)

df[["review", "Cleaned_Review"]].head()

df["Cleaned_Review_Length"] = df["Cleaned_Review"].apply(len)
df[["Cleaned_Review", "Cleaned_Review_Length"]].head()

!pip install textblob
from textblob import TextBlob

def review_sentiment_pol(text):
  pol = TextBlob(text).sentiment.polarity
  return pol

def review_sentiment_sub(text):
  sub = TextBlob(text).sentiment.subjectivity
  return sub

df['sentiment_Polarity'] = df['Cleaned_Review'].apply(review_sentiment_pol)
df['sentiment_Subjectivity'] = df['Cleaned_Review'].apply(review_sentiment_sub)

df[['Cleaned_Review', 'sentiment_Polarity', 'sentiment_Subjectivity']].head()

text_sentiment = " ".join(df["Cleaned_Review"].astype(str))

text_sentiment

import matplotlib.pyplot as plt
from wordcloud import WordCloud

cloud = WordCloud(width=700,
                  height=400,
                  background_color="white",
                  colormap = "mako").generate(text_sentiment)

plt.figure(figsize=(10,10))
plt.imshow(cloud, interpolation='bicubic')
plt.axis("off")
plt.title("Most Common Words in Reviews", fontsize=14)
plt.show()

from nltk.sentiment.vader import SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()
df[['Positive', 'Negative', 'Neutral']] = df['Cleaned_Review'].apply(lambda x: pd.Series(sia.polarity_scores(x))[['pos', 'neg', 'neu']])

df[['Cleaned_Review', 'Positive', 'Negative', 'Neutral']].head()

#Using CountVectorizer, where each column represents a word and values show its frequency.

from sklearn.feature_extraction.text import CountVectorizer

vect = CountVectorizer(max_features=1000)
vect.fit(df['Cleaned_Review'])
X = vect.transform(df['Cleaned_Review'])
df_array = X.toarray()
X_df = pd.DataFrame(df_array, columns=vect.get_feature_names_out())
X_df

wordsums = X_df.sum().sort_values(ascending=False)
print(wordsums.head(20))

import seaborn as  sns

plt.figure(figsize=(12,6))
sns.barplot(x=wordsums.head(20).values, y=wordsums.head(20).index, palette='mako')
plt.title('TOP 20 FREQUENT WORDS IN REVIEWS')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.show()

# Employing a pre-trained BERT-based model to analyze the sentiment of text.
from transformers import pipeline

sentiment_analyzer = pipeline("sentiment-analysis", device=0)

def bert_senti_analyzer(data):
  pred = sentiment_analyzer(data)[0]
  return pred

df['Bert_sentiment'] = df['Cleaned_Review'].apply(bert_senti_analyzer)

#Classify reviews into sentiment and subjectivity categories based on polarity and subjectivity scores.
def classify_polarity(score):
    if score > 0.3:
        return 'Positive'
    elif score < -0.1:
        return 'Negative'
    else:
        return 'Neutral'

def classify_subjectivity(score):
    if score > 0.7:
        return 'Subjective'
    elif score < 0.3:
        return 'Objective'
    else:
        return 'Mixed'
df['Polarity_Category'] = df['sentiment_Polarity'].apply(classify_polarity)
df['Subjectivity_Category'] = df['sentiment_Subjectivity'].apply(classify_subjectivity)

df['Bert_score'] = df['Bert_sentiment'].apply(lambda x: x['score'])
df['Bert_label'] = df['Bert_sentiment'].apply(lambda x: x['label'])

df.head()

#Visualizing and compare the distribution of subjectivity and polarity categories
fig, axes = plt.subplots(1, 2, figsize=(16, 6))


sns.countplot(ax=axes[0], data=df, x='Subjectivity_Category', palette='muted')
axes[0].set_title("COMPARISON BETWEEN INTUITION AND RATIONALITY(SUBJECTIVITY)", fontsize=14)
axes[0].set_xlabel("SUBJECTIVITY CATEGORY")
axes[0].set_ylabel("NUMBER OF REVIEWS")
axes[0].grid(alpha=0.7)

sns.countplot(ax=axes[1], data=df, x='Polarity_Category', palette='mako')
axes[1].set_title("POLARITY CATEGORIES", fontsize=14)
axes[1].set_xlabel("POLARITY CATEGORY")
axes[1].set_ylabel("NUMBER OF REVIEWS")
axes[1].grid(alpha=0.7)

plt.tight_layout()
plt.show()

print(df['Polarity_Category'].value_counts(normalize=True))
print(df["Subjectivity_Category"].value_counts(normalize=True))

#Visualizing the average VADER sentiment scores for Neutral, Positive, and Negative reviews.
sns.set_style("darkgrid")
average = df[['Neutral',"Positive",'Negative']].mean()
plt.figure(figsize=(10, 6))
sns.barplot(x=average.index, y=average.values, palette='mako')

for i, value in enumerate(average.values):
    plt.text(i, value + 0.01, f"{value:.2f}", ha='center', fontsize=12)
plt.grid(alpha = 0.7)
plt.tight_layout()
plt.title("Average VADER Sentiment Scores(SIA)")
plt.ylabel("Average Score")


plt.show()

#We are utilizing BERT because Vader and TextBlob cannot understand sarcasm, which can be conceived as neutral or mixed. Therefore, we employ BERT.
sns.set_style("darkgrid")
plt.figure(figsize=(10, 5))


counts = df['Bert_label'].value_counts()
sns.countplot(data=df, x='Bert_label', order=counts.index, palette='mako')
plt.title("BERT SENTIMENT LABEL COUNTS")
plt.xlabel("Sentiment")
plt.ylabel("Number of Reviews")
plt.grid(alpha = 0.7)
plt.show()